{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8efcaf",
   "metadata": {},
   "source": [
    "# <center> WSI Ćwiczenie nr4 - Implementacja drzew decyzyjncyh tworzonych algorytmem ID3</center>\n",
    "\n",
    "### <center> Adam Wróblewski</center>\n",
    "\n",
    "\n",
    "### Cel eksperymentów:\n",
    " Celem eksperymentów jest zbadanie jakości klasyfikatorów dla zbioru danych Cardio Vascular Disease Detection, a także zbadanie wpływu parametru głębokości na wynik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "952bc6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9b16e",
   "metadata": {},
   "source": [
    "Dyskretyzacja atrybutów wiek (age), waga (weight), wzrost (height), ciśnienie skurczowe (ap_hi), ciśnienie rozkurczowe (ap_lo): 29-65\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cdf8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv('cardio_train.csv', sep = ';')\n",
    "# param = 'ap_hi'\n",
    "# max = my_data.loc[my_data[param].idxmax()]\n",
    "# min = my_data.loc[my_data[param].idxmin()]\n",
    "# print('max: ', max[param])\n",
    "# print('min: ', min[param])\n",
    "# print(my_data.mean(axis=0))\n",
    "\n",
    "# dyskretyzacja parametru age \n",
    "conditions =[\n",
    "    my_data['age']/365 <= 40,\n",
    "    (my_data['age']/365 > 40) & (my_data['age']/365 < 52),\n",
    "    my_data['age']/365 >= 52 \n",
    "]\n",
    "labels =['young', 'middle', 'old']\n",
    "my_data['age'] = np.select(conditions, labels)\n",
    "\n",
    "# dyskretyzacja parametru weight\n",
    "conditions =[\n",
    "    my_data['weight'] <= 50,\n",
    "    (my_data['weight'] > 50) & (my_data['weight'] <= 90),\n",
    "    (my_data['weight'] > 90) & (my_data['weight'] <= 130),\n",
    "    my_data['weight'] > 130\n",
    "]\n",
    "labels =['underweight', 'normal', 'overweight', 'obese']\n",
    "my_data['weight'] = np.select(conditions, labels)\n",
    "\n",
    "# dyskretyzacja parametru height\n",
    "conditions =[\n",
    "    my_data['height'] <= 140,\n",
    "    (my_data['height'] > 140) & (my_data['height'] <= 190),\n",
    "    my_data['height'] > 190\n",
    "]\n",
    "labels =['short', 'normal', 'tall']\n",
    "my_data['height'] = np.select(conditions, labels)\n",
    "\n",
    "# dyskretyzacja parametru ap_hi\n",
    "conditions =[\n",
    "    my_data['ap_hi'] <= 100,\n",
    "    (my_data['ap_hi'] > 100) & (my_data['ap_hi'] <= 140),\n",
    "    my_data['ap_hi'] > 140\n",
    "]\n",
    "labels =['low', 'normal', 'high']\n",
    "my_data['ap_hi'] = np.select(conditions, labels)\n",
    "\n",
    "# dyskretyzacja parametru ap_lo\n",
    "conditions =[\n",
    "    my_data['ap_lo'] <= 70,\n",
    "    (my_data['ap_lo'] > 70) & (my_data['ap_lo'] <= 90),\n",
    "    my_data['ap_lo'] > 90\n",
    "]\n",
    "labels =['low', 'normal', 'high']\n",
    "my_data['ap_lo'] = np.select(conditions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdd084c",
   "metadata": {},
   "source": [
    "Podział danych na zbiór treningowy, walidacyjny, testowy dokonuję przy użyciu biblioteki sklearn.model_selection. Implementacja podziału znajduje się w funkcji \"create_train_validate_test_data\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4315d648",
   "metadata": {},
   "source": [
    "Implementacja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b456dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.children = []\n",
    "        self.value = \"\"\n",
    "        self.isLeaf = False\n",
    "        self.output = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b53f404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver():\n",
    "    \"\"\"A solver. Parameters may be passed during initialization.\"\"\"\n",
    "    def __init__(self, data_as_DataFrame, class_name):\n",
    "        self.data = data_as_DataFrame\n",
    "        self.X_data = None\n",
    "        self.y_data = None \n",
    "        self.class_values = None\n",
    "        self.class_name = class_name\n",
    "        self.tree = None\n",
    "        self.train_data = None\n",
    "        self.validate_data = None\n",
    "        self.test_data = None\n",
    "\n",
    "    def create_train_validate_test_data(self):\n",
    "        data = self.data\n",
    "        data = data.drop('id', axis=1, inplace=False)\n",
    "        train_data, rest = train_test_split(data, test_size=0.66, random_state=42)\n",
    "        validate_data, test_data = train_test_split(rest, test_size=0.5, random_state=42)\n",
    "        self.train_data = train_data\n",
    "        self.validate_data = validate_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def get_data(self, data):\n",
    "        x_data = data.drop(self.class_name, axis=1, inplace=False)\n",
    "        y_data = data[self.class_name]\n",
    "        self.X_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.class_values = sorted(list(set(self.y_data)), reverse=True)\n",
    "        return x_data, y_data\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"Returns a dictionary of hyperparameters\"\"\"\n",
    "        dict = {\n",
    "            'class_name': self.class_name,\n",
    "            'class_values': self.class_values\n",
    "        }\n",
    "        return dict\n",
    "\n",
    "\n",
    "    def calculate_entire_entropy(self, class_data):\n",
    "        class_values = list(set(class_data))\n",
    "        incydences = []\n",
    "        for value in class_values:\n",
    "            incydences.append((class_data.count(value))/len(class_data))\n",
    "\n",
    "        entire_entropy = 0\n",
    "        for incydence in incydences:\n",
    "            entropy = incydence * math.log2(incydence)\n",
    "            entire_entropy += entropy       \n",
    "        return -entire_entropy\n",
    "    \n",
    "    def calculate_entropy_for_feature(self, data): #data - te wiersze które posiadają wybraną wartość feature-a\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "        for _, row in data.iterrows():\n",
    "            if row[self.class_name] == self.class_values[0]:\n",
    "                positive_count += 1\n",
    "            else:\n",
    "                negative_count += 1\n",
    "        \n",
    "        if positive_count == 0 or negative_count == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            total = positive_count + negative_count\n",
    "            pos_incydence = positive_count/(total)\n",
    "            neg_incydence = negative_count/(total)\n",
    "            return -(pos_incydence * math.log2(pos_incydence) + neg_incydence * math.log2(neg_incydence))\n",
    "        \n",
    "    \n",
    "    def calculate_info_gain_for_feature(self, data, feature_name):\n",
    "        feature_values = sorted(set(data[feature_name]))\n",
    "        rows_count = data.shape[0]\n",
    "        info_gain = 0\n",
    "        entire_entropy = self.calculate_entire_entropy(list(data[self.class_name]))\n",
    "        info_gain += entire_entropy\n",
    "\n",
    "        for feature_value in feature_values:\n",
    "            feature_value_data = data[data[feature_name] == feature_value]\n",
    "            feature_value_incydence = feature_value_data.shape[0] / rows_count\n",
    "            feature_value_entropy = self.calculate_entropy_for_feature(feature_value_data) \n",
    "            info_gain -= feature_value_incydence * feature_value_entropy\n",
    "        return info_gain\n",
    "\n",
    "\n",
    "    def find_best_feature(self, data):\n",
    "        features = list(data.columns.drop(self.class_name))\n",
    "        best_info_gain = -100000\n",
    "        best_info_feature = None\n",
    "\n",
    "        for feature in features:\n",
    "            feature_info_gain = self.calculate_info_gain_for_feature(data, feature)\n",
    "            if feature_info_gain > best_info_gain:\n",
    "                best_info_gain = feature_info_gain\n",
    "                best_info_feature = feature\n",
    "        \n",
    "        return best_info_feature\n",
    " \n",
    "\n",
    "    def ID3(self, data, depth):\n",
    "        root = Node()\n",
    "        \n",
    "        best_feature = self.find_best_feature(data)\n",
    "        root.value = best_feature\n",
    "        feature_values = sorted(set(data[best_feature]))\n",
    "        \n",
    "        if depth == 0:\n",
    "            return None \n",
    "\n",
    "        for feature_value in feature_values:\n",
    "            feature_data = data[data[best_feature] == feature_value]\n",
    "            entropy = self.calculate_entropy_for_feature(feature_data)\n",
    "            if entropy == 0:\n",
    "                leaf_node = Node()\n",
    "                leaf_node.isLeaf = True\n",
    "                leaf_node.value = feature_value\n",
    "                leaf_node.output = list(feature_data[self.class_name].unique())[0]\n",
    "                root.children.append(leaf_node)\n",
    "            \n",
    "            else:\n",
    "                mid_node = Node()\n",
    "                mid_node.value = feature_value\n",
    "                stripped_data = feature_data.drop(best_feature, axis=1, inplace=False)\n",
    "                child = self.ID3(stripped_data, depth - 1)\n",
    "                if child != None:\n",
    "                    mid_node.children.append(child)\n",
    "                elif child == None:\n",
    "                    mid_node.isLeaf = True\n",
    "                    mid_node.output = data[self.class_name].value_counts().idxmax()\n",
    "                root.children.append(mid_node)\n",
    "        self.tree = root\n",
    "        return root\n",
    "\n",
    "\n",
    "    def print_tree(self, root: Node, depth=0):\n",
    "        for i in range(depth):\n",
    "            print(\"\\t\", end=\"\")\n",
    "        print(root.value, end=\"\")\n",
    "        \n",
    "        if root.isLeaf:\n",
    "            print(\" -> \", root.output)\n",
    "        print()\n",
    "        \n",
    "        for child in root.children:\n",
    "            self.print_tree(child, depth + 1)\n",
    "    \n",
    "\n",
    "    \n",
    "    def fit(self, X, y, depth):\n",
    "        \"\"\"\n",
    "        A method that fits the solver to the given data.\n",
    "        X is the dataset without the class attribute.\n",
    "        y contains the class attribute for each sample from X.\n",
    "        It may return anything.\n",
    "        \"\"\"\n",
    "        connected_data = pd.concat([X,y], axis=1)\n",
    "        fitted_tree = self.ID3(connected_data, depth)\n",
    "        return fitted_tree\n",
    "        \n",
    "\n",
    "    def predict(self, X, tree):\n",
    "            \"\"\"\n",
    "            A method that returns predicted class for each row of X\n",
    "            \"\"\"\n",
    "            feature_name = tree.value\n",
    "            for node in tree.children:\n",
    "                a = node.value\n",
    "                b = X[feature_name]\n",
    "                if node.value == X[feature_name]:\n",
    "                    if node.isLeaf:\n",
    "                        # print (\"Predicted output for\", X ,\" is:\", node.output)\n",
    "                        return node.output\n",
    "                    else:\n",
    "                        result = self.predict(X, node.children[0])\n",
    "                        if result != '':\n",
    "                            return result\n",
    "\n",
    "\n",
    "    def validate(self, tree, my_validate_data):       \n",
    "        incorrect_predict_count = 0\n",
    "        correct_predict_count = 0\n",
    "\n",
    "        for _, row in my_validate_data.iterrows():\n",
    "            # print(row)\n",
    "            prediction = self.predict(row, tree)\n",
    "            real_value = row[self.class_name]\n",
    "            if prediction == real_value:\n",
    "                correct_predict_count += 1\n",
    "            else:\n",
    "                incorrect_predict_count += 1\n",
    "        accuracy = correct_predict_count/(correct_predict_count+incorrect_predict_count)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb7df5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23562, 12)\n"
     ]
    }
   ],
   "source": [
    "my_solver = Solver(my_data,'cardio')\n",
    "my_solver.create_train_validate_test_data()\n",
    "X, y = my_solver.get_data(my_solver.train_data)\n",
    "print(my_solver.train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a6638a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_depth_1 = my_solver.fit(X,y,1)\n",
    "tree_depth_2 = my_solver.fit(X,y,2)\n",
    "tree_depth_3 = my_solver.fit(X,y,3)\n",
    "tree_depth_4 = my_solver.fit(X,y,4)\n",
    "tree_depth_5 = my_solver.fit(X,y,5)\n",
    "tree_depth_6 = my_solver.fit(X,y,6)\n",
    "tree_depth_7 = my_solver.fit(X,y,7)\n",
    "tree_depth_8 = my_solver.fit(X,y,8)\n",
    "tree_depth_9 = my_solver.fit(X,y,9)\n",
    "tree_depth_10 = my_solver.fit(X,y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "753d453e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dokładność dla drzewa głębokości 1 i zbioru walidacyjnego =  0.49857886221522585\n",
      "dokładność dla drzewa głębokości 2 i zbioru walidacyjnego =  0.6000262363898727\n",
      "dokładność dla drzewa głębokości 3 i zbioru walidacyjnego =  0.6521929249201976\n",
      "dokładność dla drzewa głębokości 4 i zbioru walidacyjnego =  0.6561721107175653\n",
      "dokładność dla drzewa głębokości 5 i zbioru walidacyjnego =  0.6628623901351174\n",
      "dokładność dla drzewa głębokości 6 i zbioru walidacyjnego =  0.6642179369452097\n",
      "dokładność dla drzewa głębokości 7 i zbioru walidacyjnego =  0.6682845773754865\n",
      "dokładność dla drzewa głębokości 8 i zbioru walidacyjnego =  0.6656172110717565\n",
      "dokładność dla drzewa głębokości 9 i zbioru walidacyjnego =  0.6641304823123005\n",
      "dokładność dla drzewa głębokości 10 i zbioru walidacyjnego =  0.6599763872491146\n"
     ]
    }
   ],
   "source": [
    "score_depth_1 = my_solver.validate(tree_depth_1, my_solver.validate_data)\n",
    "print('dokładność dla drzewa głębokości 1 i zbioru walidacyjnego = ', score_depth_1)\n",
    "\n",
    "score_depth_2 = my_solver.validate(tree_depth_2, my_solver.validate_data)\n",
    "print('dokładność dla drzewa głębokości 2 i zbioru walidacyjnego = ', score_depth_2)\n",
    "\n",
    "score_depth_3 = my_solver.validate(tree_depth_3, my_solver.validate_data)\n",
    "print('dokładność dla drzewa głębokości 3 i zbioru walidacyjnego = ', score_depth_3)\n",
    "\n",
    "score_depth_4 = my_solver.validate(tree_depth_4, my_solver.validate_data)\n",
    "print('dokładność dla drzewa głębokości 4 i zbioru walidacyjnego = ', score_depth_4)\n",
    "\n",
    "score_depth_5 = my_solver.validate(tree_depth_5, my_solver.validate_data)\n",
    "print('dokładność dla drzewa głębokości 5 i zbioru walidacyjnego = ', score_depth_5)\n",
    "\n",
    "score_depth_6 = my_solver.validate(tree_depth_6, my_solver.validate_data)\n",
    "print('dokładność dla drzewa głębokości 6 i zbioru walidacyjnego = ', score_depth_6)\n",
    "\n",
    "score_depth_7 = my_solver.validate(tree_depth_7, my_solver.validate_data)\n",
    "print('dokładność dla drzewa głębokości 7 i zbioru walidacyjnego = ', score_depth_7)\n",
    "\n",
    "score_depth_8 = my_solver.validate(tree_depth_8, my_solver.validate_data)\n",
    "print('dokładność dla drzewa głębokości 8 i zbioru walidacyjnego = ', score_depth_8)\n",
    "\n",
    "score_depth_9 = my_solver.validate(tree_depth_9, my_solver.validate_data)\n",
    "print('dokładność dla drzewa głębokości 9 i zbioru walidacyjnego = ', score_depth_9)\n",
    "\n",
    "score_depth_10 = my_solver.validate(tree_depth_10, my_solver.validate_data)\n",
    "print('dokładność dla drzewa głębokości 10 i zbioru walidacyjnego = ', score_depth_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e54d1c",
   "metadata": {},
   "source": [
    "Jak widzimy dokładność wyznaczania wyjścia modelu jest najlepsza dla drzewa o głębokości 6. Dla mniejszych wartości głębokości: 3, 4, 5 dokładność jest bardzo zbliżona, natomiast dla głębokości drzewa równych 1 i 2 precyzja jest znacznie mniejsza - przyczyną takiego zjawiska jest duża niedokładność modelu przy tak niskiej głębokości.\n",
    "\n",
    "Precyzja dla wartości głębogości większej od 7 nieznacznie spada, wynikać to może z nadmiernego dopasowania - przy zbyt dokładnym modelu dane inne niż trenujące powodują błędy.\n",
    "\n",
    "Z wyżej przedstawionych danych wynika że najlepszym modelem będzie drzewo decyzyjne o głębokości równej 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b142b98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dokładność dla zbioru testowego i drzewa o głębokości 7:  0.6745080891998251\n"
     ]
    }
   ],
   "source": [
    "score = my_solver.validate(tree_depth_7, my_solver.test_data)\n",
    "print('dokładność dla zbioru testowego i drzewa o głębokości 7: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a88cb0",
   "metadata": {},
   "source": [
    "Dokładność dla zbioru testowego wynosi 0,67. Wynik ten reprezentuje skuteczność modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02371cf",
   "metadata": {},
   "source": [
    "### Wnioski\n",
    "Algorytm ID3 służacy do stworzenia modelu jakim są drzewa decyzyjne jest stosunkowo prosty w implementacji.\n",
    "Skuteczność modelu przez niego generowanego zależy od wielu czynników:<br>\n",
    "• zbioru danych wejściowych.<br>\n",
    "• sposobie podziału zbioru danych na zbiory treningowy, walidacyjny, testowy.<br>\n",
    "• doboru parametrów modelu przy użyciu zbioru walidacyjnego - możliwe jest nadmierne dopasowanie, co spowoduje że dane ze zbioru testowego będą gorzej klasyfikowane, a tym samym błędy modelu będą większe.<br>\n",
    "• ilości danych wejściowych - im więcej danych tym dokładniejszy model otrzymamy.<br>\n",
    "• ilości atrybutów zbioru danych - przy bardzo dużej ilości atrybutów wygenerowany model może działać niepoprawnie, lub jego generacja moze trwać bardzo długo.<br><br>\n",
    "\n",
    "\n",
    "Wpływ głębokości drzewa na poprawność estymacji wyniku:<br>\n",
    "Generalnie im większa głębokość drzewa tym model jest dokładniejszy - jest o wiele bardziej dopsasowany do danych trenujących i estymata wyjścia jest lepsza. <br>\n",
    "Jednak przy zbyt dużych głębokościach obserwujemy nieznaczne zmniejszenie precyzji, wynika to z nadmiernego dopasowania modelu do danych trenujących co powoduje wzrost błędów modelu.\n",
    "\n",
    "Warto zaznaczyć że mimo tego że algorytm nie uwzględnia wszystkich atrybutów w drzewie decyzyjnym to przy bardzo dużej ilości danych proces generowania drzewa decyzyjnego jest długotrwały i czas potrzebny na wykonanie obliczeń znacząco się wydłuża, natomiast na potrzeby walidacji nowych danych wystarczy jednorazowo wygenerować drzewo. \n",
    "\n",
    "Zaletą jest także fakt że klasyfikacja nowych danych na podstawie modelu w postaci drzewa decyzyjnego nie wymaga długiego czasu obliczeniowego i przebiega bardzo szybko.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
