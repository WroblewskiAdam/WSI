{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> WSI Ćwiczenie nr.6 - Algorytm Q-learning</center>\n",
    "\n",
    "### <center>Adam Wróblewski</center>\n",
    "\n",
    "\n",
    "### Cel ćwiczenia, eksperymentów:\n",
    "Celem ćwiczenia jest implementacja algorytmu Q-learning, następnie stworzenie agenta rozwiązującego problem *TAXI* z biblioteki *gym* i zbadanie wpływu poszczególnych hiperparametrów na działanie tego algorytmu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import neccessary packages\n",
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize TAXI enviroment \n",
    "env = gym.make('Taxi-v3', render_mode='ansi')\n",
    "state = env.reset()\n",
    "# print(env.render())\n",
    "# taxi_row, taxi_col, passenger_index, destination_index = env.decode(state[0])\n",
    "# print(taxi_row, taxi_col, passenger_index, destination_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent class with methods responsible for learning (creating Q table) and executing simulations based on created Q table\n",
    "class Agent:\n",
    "    def __init__(self, env, learning_rate, discount, epsilon):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        dict = {\n",
    "            \"env\":self.env,\n",
    "            \"learning_rate\":self.learning_rate,\n",
    "            \"discount\": self.discount,\n",
    "            \"epsilon\": self.epsilon\n",
    "        }\n",
    "        return dict\n",
    "\n",
    "    def q_learnning(self, epochs, period_of_evaluation = 5000, num_of_evaluations = 100):\n",
    "        env = self.env\n",
    "        learning_rate = self.learning_rate\n",
    "        discount = self.discount\n",
    "        epsilon = self.epsilon\n",
    "        \n",
    "        q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        for i in range(epochs): \n",
    "            state = env.reset()\n",
    "            state = state[0]\n",
    "            terminated = False\n",
    "            while terminated == False:\n",
    "                if np.random.random() < epsilon: #exploration\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(q_table[state])\n",
    "\n",
    "                action_result = env.step(action)\n",
    "                next_state = action_result[0]\n",
    "                reward = action_result[1]\n",
    "                terminated = action_result[2]\n",
    "\n",
    "                q_prev = q_table[state, action]\n",
    "                new_q = q_prev + learning_rate*(reward + discount*max(q_table[next_state]) - q_prev)\n",
    "                new_q_1 = (1-learning_rate) * q_prev + learning_rate * (reward + discount*max(q_table[next_state]))\n",
    "                q_table[state, action] = new_q\n",
    "\n",
    "                state = next_state\n",
    "                \n",
    "            if ((i+1) % period_of_evaluation == 0):\n",
    "                avg_score = self.evaluate(q_table, num_of_evaluations)\n",
    "                print(f\"Episode {i+1}/{epochs}, avg score of {num_of_evaluations} evaluation = {avg_score}\")\n",
    "                \n",
    "        print(\"learning ended\")\n",
    "        return q_table\n",
    "    \n",
    "    \n",
    "    def evaluate(self, q_table, num_of_evaluations):\n",
    "        scores_table = []\n",
    "        for i in range(num_of_evaluations):\n",
    "            score = self.run_simulation(q_table)\n",
    "            scores_table.append(score)\n",
    "        avg_score = sum(scores_table)/len(scores_table)\n",
    "        return avg_score\n",
    "        \n",
    "    \n",
    "    def run_simulation(self, q_table, visual = False):\n",
    "        env = self.env\n",
    "                \n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "        if visual: print(env.render())\n",
    "        \n",
    "        terminated = False\n",
    "        rewards = 0\n",
    "        for i in range(100):\n",
    "            if visual: print(\"Step: \", i)\n",
    "            action = np.argmax(q_table[state])\n",
    "            action_result = env.step(action)\n",
    "            next_state = action_result[0]\n",
    "            reward = action_result[1]\n",
    "            terminated = action_result[2]\n",
    "\n",
    "            rewards += reward\n",
    "            if visual:\n",
    "                print(env.render())\n",
    "                print(\"score: \", rewards)\n",
    "            state = next_state\n",
    "\n",
    "            if terminated == True:\n",
    "                return rewards\n",
    "        return rewards\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stworzenie bazowego agenta, na podstawie którego zbadam wpływ poszczególnych parametrów na działaanie algorytmu Q learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = -28.84\n",
      "Episode 5000/100000, avg score of 100 evaluation = -2.44\n",
      "Episode 7500/100000, avg score of 100 evaluation = -0.41\n",
      "Episode 10000/100000, avg score of 100 evaluation = 2.56\n",
      "Episode 12500/100000, avg score of 100 evaluation = 8.28\n",
      "Episode 15000/100000, avg score of 100 evaluation = 4.87\n",
      "Episode 17500/100000, avg score of 100 evaluation = 6.69\n",
      "Episode 20000/100000, avg score of 100 evaluation = 7.82\n",
      "Episode 22500/100000, avg score of 100 evaluation = 7.83\n",
      "Episode 25000/100000, avg score of 100 evaluation = 8.03\n",
      "Episode 27500/100000, avg score of 100 evaluation = 7.96\n",
      "Episode 30000/100000, avg score of 100 evaluation = 8.35\n",
      "Episode 32500/100000, avg score of 100 evaluation = 7.99\n",
      "Episode 35000/100000, avg score of 100 evaluation = 7.85\n",
      "Episode 37500/100000, avg score of 100 evaluation = 7.57\n",
      "Episode 40000/100000, avg score of 100 evaluation = 7.91\n",
      "Episode 42500/100000, avg score of 100 evaluation = 8.13\n",
      "Episode 45000/100000, avg score of 100 evaluation = 7.63\n",
      "Episode 47500/100000, avg score of 100 evaluation = 7.93\n",
      "Episode 50000/100000, avg score of 100 evaluation = 7.69\n",
      "Episode 52500/100000, avg score of 100 evaluation = 7.77\n",
      "Episode 55000/100000, avg score of 100 evaluation = 8.03\n",
      "Episode 57500/100000, avg score of 100 evaluation = 8.2\n",
      "Episode 60000/100000, avg score of 100 evaluation = 7.59\n",
      "Episode 62500/100000, avg score of 100 evaluation = 8.24\n",
      "Episode 65000/100000, avg score of 100 evaluation = 7.81\n",
      "Episode 67500/100000, avg score of 100 evaluation = 7.98\n",
      "Episode 70000/100000, avg score of 100 evaluation = 7.95\n",
      "Episode 72500/100000, avg score of 100 evaluation = 7.9\n",
      "Episode 75000/100000, avg score of 100 evaluation = 7.9\n",
      "Episode 77500/100000, avg score of 100 evaluation = 7.75\n",
      "Episode 80000/100000, avg score of 100 evaluation = 7.79\n",
      "Episode 82500/100000, avg score of 100 evaluation = 8.43\n",
      "Episode 85000/100000, avg score of 100 evaluation = 7.82\n",
      "Episode 87500/100000, avg score of 100 evaluation = 8.21\n",
      "Episode 90000/100000, avg score of 100 evaluation = 8.16\n",
      "Episode 92500/100000, avg score of 100 evaluation = 7.94\n",
      "Episode 95000/100000, avg score of 100 evaluation = 7.67\n",
      "Episode 97500/100000, avg score of 100 evaluation = 7.95\n",
      "Episode 100000/100000, avg score of 100 evaluation = 7.91\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent1 = Agent(env=env, learning_rate=0.1, discount=0.7, epsilon=0.1)\n",
    "my_q_table1 = agent1.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)\n",
    "# agent.run_simulation(q_table=my_q_table, visual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zbadanie wpływu parametru learning rate:\n",
    "Parametr ten jest często oznaczany jako $\\beta$  lub $\\alpha$, mówi on o tym jak szybko algorytm będzie zmieniał wartości w tablicy Q, innymi słowy, jak ważne są nowo zdobyte informacje o środowisku względem tych które już posiada. </br>\n",
    "Zwiększenie go, sprawi że wartości w tablicy Q będą zmieniały się szybciej(bardziej), co powinno skutkować szybszym uczeniem się agenta, i tym samym osiągnięciem satysfakcjonującego wyniku w mniejszej liczbie iteracji. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = 0.55\n",
      "Episode 5000/100000, avg score of 100 evaluation = 7.99\n",
      "Episode 7500/100000, avg score of 100 evaluation = 7.22\n",
      "Episode 10000/100000, avg score of 100 evaluation = 8.04\n",
      "Episode 12500/100000, avg score of 100 evaluation = 7.79\n",
      "Episode 15000/100000, avg score of 100 evaluation = 7.67\n",
      "Episode 17500/100000, avg score of 100 evaluation = 8.12\n",
      "Episode 20000/100000, avg score of 100 evaluation = 7.87\n",
      "Episode 22500/100000, avg score of 100 evaluation = 7.81\n",
      "Episode 25000/100000, avg score of 100 evaluation = 7.86\n",
      "Episode 27500/100000, avg score of 100 evaluation = 8.0\n",
      "Episode 30000/100000, avg score of 100 evaluation = 7.52\n",
      "Episode 32500/100000, avg score of 100 evaluation = 8.06\n",
      "Episode 35000/100000, avg score of 100 evaluation = 8.04\n",
      "Episode 37500/100000, avg score of 100 evaluation = 8.09\n",
      "Episode 40000/100000, avg score of 100 evaluation = 8.04\n",
      "Episode 42500/100000, avg score of 100 evaluation = 8.09\n",
      "Episode 45000/100000, avg score of 100 evaluation = 8.16\n",
      "Episode 47500/100000, avg score of 100 evaluation = 8.01\n",
      "Episode 50000/100000, avg score of 100 evaluation = 8.32\n",
      "Episode 52500/100000, avg score of 100 evaluation = 8.06\n",
      "Episode 55000/100000, avg score of 100 evaluation = 8.24\n",
      "Episode 57500/100000, avg score of 100 evaluation = 8.01\n",
      "Episode 60000/100000, avg score of 100 evaluation = 7.77\n",
      "Episode 62500/100000, avg score of 100 evaluation = 8.02\n",
      "Episode 65000/100000, avg score of 100 evaluation = 8.01\n",
      "Episode 67500/100000, avg score of 100 evaluation = 7.66\n",
      "Episode 70000/100000, avg score of 100 evaluation = 8.11\n",
      "Episode 72500/100000, avg score of 100 evaluation = 7.91\n",
      "Episode 75000/100000, avg score of 100 evaluation = 7.98\n",
      "Episode 77500/100000, avg score of 100 evaluation = 7.64\n",
      "Episode 80000/100000, avg score of 100 evaluation = 8.23\n",
      "Episode 82500/100000, avg score of 100 evaluation = 8.25\n",
      "Episode 85000/100000, avg score of 100 evaluation = 8.13\n",
      "Episode 87500/100000, avg score of 100 evaluation = 8.1\n",
      "Episode 90000/100000, avg score of 100 evaluation = 8.17\n",
      "Episode 92500/100000, avg score of 100 evaluation = 7.94\n",
      "Episode 95000/100000, avg score of 100 evaluation = 7.59\n",
      "Episode 97500/100000, avg score of 100 evaluation = 8.29\n",
      "Episode 100000/100000, avg score of 100 evaluation = 7.99\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent2 = Agent(env=env, learning_rate=0.3, discount=0.7, epsilon=0.1)\n",
    "my_q_table2 = agent2.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = 5.62\n",
      "Episode 5000/100000, avg score of 100 evaluation = 7.9\n",
      "Episode 7500/100000, avg score of 100 evaluation = 7.74\n",
      "Episode 10000/100000, avg score of 100 evaluation = 7.63\n",
      "Episode 12500/100000, avg score of 100 evaluation = 7.97\n",
      "Episode 15000/100000, avg score of 100 evaluation = 7.5\n",
      "Episode 17500/100000, avg score of 100 evaluation = 7.75\n",
      "Episode 20000/100000, avg score of 100 evaluation = 7.57\n",
      "Episode 22500/100000, avg score of 100 evaluation = 7.69\n",
      "Episode 25000/100000, avg score of 100 evaluation = 8.03\n",
      "Episode 27500/100000, avg score of 100 evaluation = 7.98\n",
      "Episode 30000/100000, avg score of 100 evaluation = 8.4\n",
      "Episode 32500/100000, avg score of 100 evaluation = 7.09\n",
      "Episode 35000/100000, avg score of 100 evaluation = 8.16\n",
      "Episode 37500/100000, avg score of 100 evaluation = 8.16\n",
      "Episode 40000/100000, avg score of 100 evaluation = 7.94\n",
      "Episode 42500/100000, avg score of 100 evaluation = 8.04\n",
      "Episode 45000/100000, avg score of 100 evaluation = 8.39\n",
      "Episode 47500/100000, avg score of 100 evaluation = 7.94\n",
      "Episode 50000/100000, avg score of 100 evaluation = 7.7\n",
      "Episode 52500/100000, avg score of 100 evaluation = 7.73\n",
      "Episode 55000/100000, avg score of 100 evaluation = 7.68\n",
      "Episode 57500/100000, avg score of 100 evaluation = 7.57\n",
      "Episode 60000/100000, avg score of 100 evaluation = 7.96\n",
      "Episode 62500/100000, avg score of 100 evaluation = 7.88\n",
      "Episode 65000/100000, avg score of 100 evaluation = 7.8\n",
      "Episode 67500/100000, avg score of 100 evaluation = 8.06\n",
      "Episode 70000/100000, avg score of 100 evaluation = 8.66\n",
      "Episode 72500/100000, avg score of 100 evaluation = 7.81\n",
      "Episode 75000/100000, avg score of 100 evaluation = 7.77\n",
      "Episode 77500/100000, avg score of 100 evaluation = 7.99\n",
      "Episode 80000/100000, avg score of 100 evaluation = 7.64\n",
      "Episode 82500/100000, avg score of 100 evaluation = 7.55\n",
      "Episode 85000/100000, avg score of 100 evaluation = 7.87\n",
      "Episode 87500/100000, avg score of 100 evaluation = 7.92\n",
      "Episode 90000/100000, avg score of 100 evaluation = 7.86\n",
      "Episode 92500/100000, avg score of 100 evaluation = 7.5\n",
      "Episode 95000/100000, avg score of 100 evaluation = 7.95\n",
      "Episode 97500/100000, avg score of 100 evaluation = 8.02\n",
      "Episode 100000/100000, avg score of 100 evaluation = 8.24\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent3 = Agent(env=env, learning_rate=0.5, discount=0.7, epsilon=0.1)\n",
    "my_q_table3 = agent3.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = 7.7\n",
      "Episode 5000/100000, avg score of 100 evaluation = 7.87\n",
      "Episode 7500/100000, avg score of 100 evaluation = 7.66\n",
      "Episode 10000/100000, avg score of 100 evaluation = 7.81\n",
      "Episode 12500/100000, avg score of 100 evaluation = 7.86\n",
      "Episode 15000/100000, avg score of 100 evaluation = 7.98\n",
      "Episode 17500/100000, avg score of 100 evaluation = 8.09\n",
      "Episode 20000/100000, avg score of 100 evaluation = 8.21\n",
      "Episode 22500/100000, avg score of 100 evaluation = 8.17\n",
      "Episode 25000/100000, avg score of 100 evaluation = 7.99\n",
      "Episode 27500/100000, avg score of 100 evaluation = 8.17\n",
      "Episode 30000/100000, avg score of 100 evaluation = 7.93\n",
      "Episode 32500/100000, avg score of 100 evaluation = 7.8\n",
      "Episode 35000/100000, avg score of 100 evaluation = 7.95\n",
      "Episode 37500/100000, avg score of 100 evaluation = 7.95\n",
      "Episode 40000/100000, avg score of 100 evaluation = 8.25\n",
      "Episode 42500/100000, avg score of 100 evaluation = 8.11\n",
      "Episode 45000/100000, avg score of 100 evaluation = 7.78\n",
      "Episode 47500/100000, avg score of 100 evaluation = 7.74\n",
      "Episode 50000/100000, avg score of 100 evaluation = 7.86\n",
      "Episode 52500/100000, avg score of 100 evaluation = 7.94\n",
      "Episode 55000/100000, avg score of 100 evaluation = 7.7\n",
      "Episode 57500/100000, avg score of 100 evaluation = 7.8\n",
      "Episode 60000/100000, avg score of 100 evaluation = 7.99\n",
      "Episode 62500/100000, avg score of 100 evaluation = 7.95\n",
      "Episode 65000/100000, avg score of 100 evaluation = 7.78\n",
      "Episode 67500/100000, avg score of 100 evaluation = 8.13\n",
      "Episode 70000/100000, avg score of 100 evaluation = 8.16\n",
      "Episode 72500/100000, avg score of 100 evaluation = 8.04\n",
      "Episode 75000/100000, avg score of 100 evaluation = 7.95\n",
      "Episode 77500/100000, avg score of 100 evaluation = 7.75\n",
      "Episode 80000/100000, avg score of 100 evaluation = 7.86\n",
      "Episode 82500/100000, avg score of 100 evaluation = 8.06\n",
      "Episode 85000/100000, avg score of 100 evaluation = 7.58\n",
      "Episode 87500/100000, avg score of 100 evaluation = 8.21\n",
      "Episode 90000/100000, avg score of 100 evaluation = 7.4\n",
      "Episode 92500/100000, avg score of 100 evaluation = 8.02\n",
      "Episode 95000/100000, avg score of 100 evaluation = 7.69\n",
      "Episode 97500/100000, avg score of 100 evaluation = 7.71\n",
      "Episode 100000/100000, avg score of 100 evaluation = 7.89\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent4 = Agent(env=env, learning_rate=0.7, discount=0.7, epsilon=0.1)\n",
    "my_q_table4 = agent4.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = 7.76\n",
      "Episode 5000/100000, avg score of 100 evaluation = 7.49\n",
      "Episode 7500/100000, avg score of 100 evaluation = 7.59\n",
      "Episode 10000/100000, avg score of 100 evaluation = 7.92\n",
      "Episode 12500/100000, avg score of 100 evaluation = 7.59\n",
      "Episode 15000/100000, avg score of 100 evaluation = 7.96\n",
      "Episode 17500/100000, avg score of 100 evaluation = 7.77\n",
      "Episode 20000/100000, avg score of 100 evaluation = 8.32\n",
      "Episode 22500/100000, avg score of 100 evaluation = 8.19\n",
      "Episode 25000/100000, avg score of 100 evaluation = 8.13\n",
      "Episode 27500/100000, avg score of 100 evaluation = 8.31\n",
      "Episode 30000/100000, avg score of 100 evaluation = 8.05\n",
      "Episode 32500/100000, avg score of 100 evaluation = 7.58\n",
      "Episode 35000/100000, avg score of 100 evaluation = 7.86\n",
      "Episode 37500/100000, avg score of 100 evaluation = 8.53\n",
      "Episode 40000/100000, avg score of 100 evaluation = 7.56\n",
      "Episode 42500/100000, avg score of 100 evaluation = 7.98\n",
      "Episode 45000/100000, avg score of 100 evaluation = 7.8\n",
      "Episode 47500/100000, avg score of 100 evaluation = 8.16\n",
      "Episode 50000/100000, avg score of 100 evaluation = 7.79\n",
      "Episode 52500/100000, avg score of 100 evaluation = 8.17\n",
      "Episode 55000/100000, avg score of 100 evaluation = 8.33\n",
      "Episode 57500/100000, avg score of 100 evaluation = 7.64\n",
      "Episode 60000/100000, avg score of 100 evaluation = 7.73\n",
      "Episode 62500/100000, avg score of 100 evaluation = 8.03\n",
      "Episode 65000/100000, avg score of 100 evaluation = 7.8\n",
      "Episode 67500/100000, avg score of 100 evaluation = 7.53\n",
      "Episode 70000/100000, avg score of 100 evaluation = 7.47\n",
      "Episode 72500/100000, avg score of 100 evaluation = 8.2\n",
      "Episode 75000/100000, avg score of 100 evaluation = 8.07\n",
      "Episode 77500/100000, avg score of 100 evaluation = 8.41\n",
      "Episode 80000/100000, avg score of 100 evaluation = 7.43\n",
      "Episode 82500/100000, avg score of 100 evaluation = 8.19\n",
      "Episode 85000/100000, avg score of 100 evaluation = 7.82\n",
      "Episode 87500/100000, avg score of 100 evaluation = 7.85\n",
      "Episode 90000/100000, avg score of 100 evaluation = 7.85\n",
      "Episode 92500/100000, avg score of 100 evaluation = 7.94\n",
      "Episode 95000/100000, avg score of 100 evaluation = 8.17\n",
      "Episode 97500/100000, avg score of 100 evaluation = 7.47\n",
      "Episode 100000/100000, avg score of 100 evaluation = 7.56\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent5 = Agent(env=env, learning_rate=0.9, discount=0.7, epsilon=0.1)\n",
    "my_q_table5 = agent5.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zbadanie wpływu parametru discount:\n",
    "Parametr ten jest często oznaczany jako $\\gamma$, mówi on o tym jak bardzo mają być brane pod uwagę nagrody które agent może otrzymać w przyszłości względem tych które otrzymuje \"natychmiast\" - w danym kroku.</br> Duża wartość $\\gamma$ oznacza że przyszłe nagrody są bardzo ważne, natmiast w miarę zmniejszania tego parametru przyszłe nagrody jakie agent może otrzymać mają mniejszy wpływ, za to nagrody otrzymywane w danym kroku będą bardziej znaczące."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = -29.3\n",
      "Episode 5000/100000, avg score of 100 evaluation = -6.9\n",
      "Episode 7500/100000, avg score of 100 evaluation = 1.98\n",
      "Episode 10000/100000, avg score of 100 evaluation = 3.83\n",
      "Episode 12500/100000, avg score of 100 evaluation = 3.94\n",
      "Episode 15000/100000, avg score of 100 evaluation = 4.62\n",
      "Episode 17500/100000, avg score of 100 evaluation = 7.02\n",
      "Episode 20000/100000, avg score of 100 evaluation = 5.86\n",
      "Episode 22500/100000, avg score of 100 evaluation = 7.57\n",
      "Episode 25000/100000, avg score of 100 evaluation = 7.49\n",
      "Episode 27500/100000, avg score of 100 evaluation = 7.99\n",
      "Episode 30000/100000, avg score of 100 evaluation = 7.82\n",
      "Episode 32500/100000, avg score of 100 evaluation = 7.8\n",
      "Episode 35000/100000, avg score of 100 evaluation = 7.7\n",
      "Episode 37500/100000, avg score of 100 evaluation = 7.77\n",
      "Episode 40000/100000, avg score of 100 evaluation = 7.87\n",
      "Episode 42500/100000, avg score of 100 evaluation = 7.39\n",
      "Episode 45000/100000, avg score of 100 evaluation = 7.51\n",
      "Episode 47500/100000, avg score of 100 evaluation = 7.95\n",
      "Episode 50000/100000, avg score of 100 evaluation = 7.42\n",
      "Episode 52500/100000, avg score of 100 evaluation = 7.92\n",
      "Episode 55000/100000, avg score of 100 evaluation = 8.32\n",
      "Episode 57500/100000, avg score of 100 evaluation = 7.93\n",
      "Episode 60000/100000, avg score of 100 evaluation = 7.92\n",
      "Episode 62500/100000, avg score of 100 evaluation = 7.97\n",
      "Episode 65000/100000, avg score of 100 evaluation = 7.83\n",
      "Episode 67500/100000, avg score of 100 evaluation = 7.84\n",
      "Episode 70000/100000, avg score of 100 evaluation = 8.36\n",
      "Episode 72500/100000, avg score of 100 evaluation = 8.26\n",
      "Episode 75000/100000, avg score of 100 evaluation = 8.09\n",
      "Episode 77500/100000, avg score of 100 evaluation = 8.14\n",
      "Episode 80000/100000, avg score of 100 evaluation = 7.5\n",
      "Episode 82500/100000, avg score of 100 evaluation = 8.13\n",
      "Episode 85000/100000, avg score of 100 evaluation = 7.73\n",
      "Episode 87500/100000, avg score of 100 evaluation = 7.92\n",
      "Episode 90000/100000, avg score of 100 evaluation = 8.2\n",
      "Episode 92500/100000, avg score of 100 evaluation = 7.78\n",
      "Episode 95000/100000, avg score of 100 evaluation = 7.8\n",
      "Episode 97500/100000, avg score of 100 evaluation = 7.86\n",
      "Episode 100000/100000, avg score of 100 evaluation = 7.74\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent10 = Agent(env=env, learning_rate=0.1, discount=0.7, epsilon=0.1)\n",
    "my_q_table11 = agent10.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = 2.72\n",
      "Episode 5000/100000, avg score of 100 evaluation = 7.92\n",
      "Episode 7500/100000, avg score of 100 evaluation = 7.83\n",
      "Episode 10000/100000, avg score of 100 evaluation = 7.7\n",
      "Episode 12500/100000, avg score of 100 evaluation = 7.98\n",
      "Episode 15000/100000, avg score of 100 evaluation = 7.59\n",
      "Episode 17500/100000, avg score of 100 evaluation = 8.19\n",
      "Episode 20000/100000, avg score of 100 evaluation = 7.78\n",
      "Episode 22500/100000, avg score of 100 evaluation = 8.2\n",
      "Episode 25000/100000, avg score of 100 evaluation = 7.57\n",
      "Episode 27500/100000, avg score of 100 evaluation = 7.74\n",
      "Episode 30000/100000, avg score of 100 evaluation = 7.79\n",
      "Episode 32500/100000, avg score of 100 evaluation = 8.03\n",
      "Episode 35000/100000, avg score of 100 evaluation = 7.91\n",
      "Episode 37500/100000, avg score of 100 evaluation = 7.91\n",
      "Episode 40000/100000, avg score of 100 evaluation = 7.79\n",
      "Episode 42500/100000, avg score of 100 evaluation = 7.86\n",
      "Episode 45000/100000, avg score of 100 evaluation = 8.07\n",
      "Episode 47500/100000, avg score of 100 evaluation = 8.04\n",
      "Episode 50000/100000, avg score of 100 evaluation = 8.03\n",
      "Episode 52500/100000, avg score of 100 evaluation = 7.65\n",
      "Episode 55000/100000, avg score of 100 evaluation = 8.08\n",
      "Episode 57500/100000, avg score of 100 evaluation = 7.54\n",
      "Episode 60000/100000, avg score of 100 evaluation = 8.0\n",
      "Episode 62500/100000, avg score of 100 evaluation = 7.96\n",
      "Episode 65000/100000, avg score of 100 evaluation = 7.85\n",
      "Episode 67500/100000, avg score of 100 evaluation = 7.67\n",
      "Episode 70000/100000, avg score of 100 evaluation = 8.26\n",
      "Episode 72500/100000, avg score of 100 evaluation = 7.62\n",
      "Episode 75000/100000, avg score of 100 evaluation = 8.24\n",
      "Episode 77500/100000, avg score of 100 evaluation = 7.95\n",
      "Episode 80000/100000, avg score of 100 evaluation = 7.55\n",
      "Episode 82500/100000, avg score of 100 evaluation = 7.93\n",
      "Episode 85000/100000, avg score of 100 evaluation = 7.59\n",
      "Episode 87500/100000, avg score of 100 evaluation = 7.66\n",
      "Episode 90000/100000, avg score of 100 evaluation = 8.0\n",
      "Episode 92500/100000, avg score of 100 evaluation = 8.0\n",
      "Episode 95000/100000, avg score of 100 evaluation = 7.47\n",
      "Episode 97500/100000, avg score of 100 evaluation = 7.67\n",
      "Episode 100000/100000, avg score of 100 evaluation = 8.09\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent11 = Agent(env=env, learning_rate=0.1, discount=0.9, epsilon=0.1)\n",
    "my_q_table11 = agent11.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = -50.48\n",
      "Episode 5000/100000, avg score of 100 evaluation = -30.19\n",
      "Episode 7500/100000, avg score of 100 evaluation = -4.68\n",
      "Episode 10000/100000, avg score of 100 evaluation = -5.73\n",
      "Episode 12500/100000, avg score of 100 evaluation = 0.29\n",
      "Episode 15000/100000, avg score of 100 evaluation = -1.13\n",
      "Episode 17500/100000, avg score of 100 evaluation = 2.59\n",
      "Episode 20000/100000, avg score of 100 evaluation = 4.9\n",
      "Episode 22500/100000, avg score of 100 evaluation = 8.09\n",
      "Episode 25000/100000, avg score of 100 evaluation = 7.25\n",
      "Episode 27500/100000, avg score of 100 evaluation = 6.82\n",
      "Episode 30000/100000, avg score of 100 evaluation = 8.0\n",
      "Episode 32500/100000, avg score of 100 evaluation = 5.68\n",
      "Episode 35000/100000, avg score of 100 evaluation = 7.87\n",
      "Episode 37500/100000, avg score of 100 evaluation = 7.94\n",
      "Episode 40000/100000, avg score of 100 evaluation = 7.79\n",
      "Episode 42500/100000, avg score of 100 evaluation = 7.76\n",
      "Episode 45000/100000, avg score of 100 evaluation = 8.05\n",
      "Episode 47500/100000, avg score of 100 evaluation = 7.56\n",
      "Episode 50000/100000, avg score of 100 evaluation = 8.27\n",
      "Episode 52500/100000, avg score of 100 evaluation = 7.63\n",
      "Episode 55000/100000, avg score of 100 evaluation = 7.46\n",
      "Episode 57500/100000, avg score of 100 evaluation = 7.77\n",
      "Episode 60000/100000, avg score of 100 evaluation = 7.65\n",
      "Episode 62500/100000, avg score of 100 evaluation = 7.84\n",
      "Episode 65000/100000, avg score of 100 evaluation = 8.13\n",
      "Episode 67500/100000, avg score of 100 evaluation = 8.23\n",
      "Episode 70000/100000, avg score of 100 evaluation = 7.67\n",
      "Episode 72500/100000, avg score of 100 evaluation = 7.92\n",
      "Episode 75000/100000, avg score of 100 evaluation = 7.58\n",
      "Episode 77500/100000, avg score of 100 evaluation = 7.87\n",
      "Episode 80000/100000, avg score of 100 evaluation = 7.48\n",
      "Episode 82500/100000, avg score of 100 evaluation = 8.48\n",
      "Episode 85000/100000, avg score of 100 evaluation = 7.9\n",
      "Episode 87500/100000, avg score of 100 evaluation = 8.27\n",
      "Episode 90000/100000, avg score of 100 evaluation = 8.15\n",
      "Episode 92500/100000, avg score of 100 evaluation = 7.41\n",
      "Episode 95000/100000, avg score of 100 evaluation = 7.87\n",
      "Episode 97500/100000, avg score of 100 evaluation = 7.65\n",
      "Episode 100000/100000, avg score of 100 evaluation = 7.88\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent12 = Agent(env=env, learning_rate=0.1, discount=0.5, epsilon=0.1)\n",
    "my_q_table12 = agent12.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = -53.99\n",
      "Episode 5000/100000, avg score of 100 evaluation = -45.74\n",
      "Episode 7500/100000, avg score of 100 evaluation = -23.48\n",
      "Episode 10000/100000, avg score of 100 evaluation = -3.75\n",
      "Episode 12500/100000, avg score of 100 evaluation = 1.54\n",
      "Episode 15000/100000, avg score of 100 evaluation = -3.85\n",
      "Episode 17500/100000, avg score of 100 evaluation = 1.56\n",
      "Episode 20000/100000, avg score of 100 evaluation = 1.31\n",
      "Episode 22500/100000, avg score of 100 evaluation = 6.27\n",
      "Episode 25000/100000, avg score of 100 evaluation = 7.34\n",
      "Episode 27500/100000, avg score of 100 evaluation = 7.74\n",
      "Episode 30000/100000, avg score of 100 evaluation = 7.75\n",
      "Episode 32500/100000, avg score of 100 evaluation = 6.85\n",
      "Episode 35000/100000, avg score of 100 evaluation = 7.81\n",
      "Episode 37500/100000, avg score of 100 evaluation = 7.83\n",
      "Episode 40000/100000, avg score of 100 evaluation = 7.87\n",
      "Episode 42500/100000, avg score of 100 evaluation = 7.19\n",
      "Episode 45000/100000, avg score of 100 evaluation = 7.71\n",
      "Episode 47500/100000, avg score of 100 evaluation = 7.97\n",
      "Episode 50000/100000, avg score of 100 evaluation = 7.93\n",
      "Episode 52500/100000, avg score of 100 evaluation = 8.19\n",
      "Episode 55000/100000, avg score of 100 evaluation = 8.05\n",
      "Episode 57500/100000, avg score of 100 evaluation = 8.16\n",
      "Episode 60000/100000, avg score of 100 evaluation = 8.23\n",
      "Episode 62500/100000, avg score of 100 evaluation = 8.02\n",
      "Episode 65000/100000, avg score of 100 evaluation = 8.32\n",
      "Episode 67500/100000, avg score of 100 evaluation = 8.34\n",
      "Episode 70000/100000, avg score of 100 evaluation = 7.29\n",
      "Episode 72500/100000, avg score of 100 evaluation = 8.13\n",
      "Episode 75000/100000, avg score of 100 evaluation = 8.48\n",
      "Episode 77500/100000, avg score of 100 evaluation = 7.85\n",
      "Episode 80000/100000, avg score of 100 evaluation = 7.92\n",
      "Episode 82500/100000, avg score of 100 evaluation = 8.42\n",
      "Episode 85000/100000, avg score of 100 evaluation = 7.63\n",
      "Episode 87500/100000, avg score of 100 evaluation = 8.02\n",
      "Episode 90000/100000, avg score of 100 evaluation = 8.14\n",
      "Episode 92500/100000, avg score of 100 evaluation = 7.75\n",
      "Episode 95000/100000, avg score of 100 evaluation = 8.24\n",
      "Episode 97500/100000, avg score of 100 evaluation = 8.35\n",
      "Episode 100000/100000, avg score of 100 evaluation = 8.1\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent13 = Agent(env=env, learning_rate=0.1, discount=0.3, epsilon=0.1)\n",
    "my_q_table13 = agent13.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = -70.18\n",
      "Episode 5000/100000, avg score of 100 evaluation = -43.16\n",
      "Episode 7500/100000, avg score of 100 evaluation = -22.79\n",
      "Episode 10000/100000, avg score of 100 evaluation = -15.54\n",
      "Episode 12500/100000, avg score of 100 evaluation = -4.01\n",
      "Episode 15000/100000, avg score of 100 evaluation = 0.73\n",
      "Episode 17500/100000, avg score of 100 evaluation = 2.58\n",
      "Episode 20000/100000, avg score of 100 evaluation = -0.49\n",
      "Episode 22500/100000, avg score of 100 evaluation = -0.71\n",
      "Episode 25000/100000, avg score of 100 evaluation = 7.16\n",
      "Episode 27500/100000, avg score of 100 evaluation = 4.56\n",
      "Episode 30000/100000, avg score of 100 evaluation = 4.91\n",
      "Episode 32500/100000, avg score of 100 evaluation = 5.57\n",
      "Episode 35000/100000, avg score of 100 evaluation = 6.02\n",
      "Episode 37500/100000, avg score of 100 evaluation = 5.89\n",
      "Episode 40000/100000, avg score of 100 evaluation = 4.82\n",
      "Episode 42500/100000, avg score of 100 evaluation = 5.94\n",
      "Episode 45000/100000, avg score of 100 evaluation = 5.97\n",
      "Episode 47500/100000, avg score of 100 evaluation = 5.48\n",
      "Episode 50000/100000, avg score of 100 evaluation = 5.87\n",
      "Episode 52500/100000, avg score of 100 evaluation = 6.74\n",
      "Episode 55000/100000, avg score of 100 evaluation = 6.68\n",
      "Episode 57500/100000, avg score of 100 evaluation = 4.34\n",
      "Episode 60000/100000, avg score of 100 evaluation = 6.85\n",
      "Episode 62500/100000, avg score of 100 evaluation = 5.12\n",
      "Episode 65000/100000, avg score of 100 evaluation = 3.52\n",
      "Episode 67500/100000, avg score of 100 evaluation = 5.82\n",
      "Episode 70000/100000, avg score of 100 evaluation = 6.5\n",
      "Episode 72500/100000, avg score of 100 evaluation = 5.97\n",
      "Episode 75000/100000, avg score of 100 evaluation = 6.77\n",
      "Episode 77500/100000, avg score of 100 evaluation = 7.71\n",
      "Episode 80000/100000, avg score of 100 evaluation = 5.18\n",
      "Episode 82500/100000, avg score of 100 evaluation = 8.2\n",
      "Episode 85000/100000, avg score of 100 evaluation = 5.91\n",
      "Episode 87500/100000, avg score of 100 evaluation = 7.08\n",
      "Episode 90000/100000, avg score of 100 evaluation = 8.04\n",
      "Episode 92500/100000, avg score of 100 evaluation = 6.77\n",
      "Episode 95000/100000, avg score of 100 evaluation = 8.14\n",
      "Episode 97500/100000, avg score of 100 evaluation = 6.81\n",
      "Episode 100000/100000, avg score of 100 evaluation = 6.76\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent14 = Agent(env=env, learning_rate=0.1, discount=0.1, epsilon=0.1)\n",
    "my_q_table14 = agent14.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zbadanie wpływu parametru epsilon:\n",
    "Parametr ten wskazuje na prawdopodobieństwo z jakim ma być wybrana losowa akcja spośród wszystkich dostępnych względem wyboru najlepszej akcji wynikającej z tablicy Q. Im większa wartość parametry *epsilon* tym agent bardziej eksploruje ( wybiera akcję losowo i tym samym poznaje środowsko), natomiast mała wartość będzie oznaczać eksploatację - podążeanie względem strategii zapisanej w tablicy Q. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = -24.95\n",
      "Episode 5000/100000, avg score of 100 evaluation = -6.86\n",
      "Episode 7500/100000, avg score of 100 evaluation = -3.68\n",
      "Episode 10000/100000, avg score of 100 evaluation = 3.88\n",
      "Episode 12500/100000, avg score of 100 evaluation = 1.31\n",
      "Episode 15000/100000, avg score of 100 evaluation = 4.81\n",
      "Episode 17500/100000, avg score of 100 evaluation = 7.2\n",
      "Episode 20000/100000, avg score of 100 evaluation = 7.03\n",
      "Episode 22500/100000, avg score of 100 evaluation = 6.59\n",
      "Episode 25000/100000, avg score of 100 evaluation = 7.69\n",
      "Episode 27500/100000, avg score of 100 evaluation = 7.4\n",
      "Episode 30000/100000, avg score of 100 evaluation = 7.89\n",
      "Episode 32500/100000, avg score of 100 evaluation = 8.1\n",
      "Episode 35000/100000, avg score of 100 evaluation = 8.01\n",
      "Episode 37500/100000, avg score of 100 evaluation = 7.74\n",
      "Episode 40000/100000, avg score of 100 evaluation = 7.85\n",
      "Episode 42500/100000, avg score of 100 evaluation = 8.13\n",
      "Episode 45000/100000, avg score of 100 evaluation = 7.78\n",
      "Episode 47500/100000, avg score of 100 evaluation = 8.08\n",
      "Episode 50000/100000, avg score of 100 evaluation = 8.08\n",
      "Episode 52500/100000, avg score of 100 evaluation = 7.85\n",
      "Episode 55000/100000, avg score of 100 evaluation = 7.87\n",
      "Episode 57500/100000, avg score of 100 evaluation = 7.78\n",
      "Episode 60000/100000, avg score of 100 evaluation = 7.82\n",
      "Episode 62500/100000, avg score of 100 evaluation = 7.7\n",
      "Episode 65000/100000, avg score of 100 evaluation = 7.42\n",
      "Episode 67500/100000, avg score of 100 evaluation = 8.16\n",
      "Episode 70000/100000, avg score of 100 evaluation = 7.76\n",
      "Episode 72500/100000, avg score of 100 evaluation = 7.97\n",
      "Episode 75000/100000, avg score of 100 evaluation = 7.95\n",
      "Episode 77500/100000, avg score of 100 evaluation = 7.47\n",
      "Episode 80000/100000, avg score of 100 evaluation = 7.83\n",
      "Episode 82500/100000, avg score of 100 evaluation = 7.89\n",
      "Episode 85000/100000, avg score of 100 evaluation = 7.95\n",
      "Episode 87500/100000, avg score of 100 evaluation = 8.18\n",
      "Episode 90000/100000, avg score of 100 evaluation = 7.59\n",
      "Episode 92500/100000, avg score of 100 evaluation = 8.02\n",
      "Episode 95000/100000, avg score of 100 evaluation = 7.67\n",
      "Episode 97500/100000, avg score of 100 evaluation = 7.75\n",
      "Episode 100000/100000, avg score of 100 evaluation = 8.13\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent110 = Agent(env=env, learning_rate=0.1, discount=0.7, epsilon=0.1)\n",
    "my_q_table110 = agent110.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = -18.41\n",
      "Episode 5000/100000, avg score of 100 evaluation = -8.86\n",
      "Episode 7500/100000, avg score of 100 evaluation = 1.83\n",
      "Episode 10000/100000, avg score of 100 evaluation = 4.85\n",
      "Episode 12500/100000, avg score of 100 evaluation = 3.03\n",
      "Episode 15000/100000, avg score of 100 evaluation = 6.0\n",
      "Episode 17500/100000, avg score of 100 evaluation = 8.22\n",
      "Episode 20000/100000, avg score of 100 evaluation = 7.98\n",
      "Episode 22500/100000, avg score of 100 evaluation = 8.02\n",
      "Episode 25000/100000, avg score of 100 evaluation = 7.73\n",
      "Episode 27500/100000, avg score of 100 evaluation = 7.94\n",
      "Episode 30000/100000, avg score of 100 evaluation = 7.94\n",
      "Episode 32500/100000, avg score of 100 evaluation = 7.74\n",
      "Episode 35000/100000, avg score of 100 evaluation = 7.77\n",
      "Episode 37500/100000, avg score of 100 evaluation = 8.16\n",
      "Episode 40000/100000, avg score of 100 evaluation = 7.66\n",
      "Episode 42500/100000, avg score of 100 evaluation = 7.82\n",
      "Episode 45000/100000, avg score of 100 evaluation = 8.33\n",
      "Episode 47500/100000, avg score of 100 evaluation = 8.01\n",
      "Episode 50000/100000, avg score of 100 evaluation = 7.63\n",
      "Episode 52500/100000, avg score of 100 evaluation = 8.24\n",
      "Episode 55000/100000, avg score of 100 evaluation = 8.27\n",
      "Episode 57500/100000, avg score of 100 evaluation = 8.06\n",
      "Episode 60000/100000, avg score of 100 evaluation = 8.01\n",
      "Episode 62500/100000, avg score of 100 evaluation = 7.69\n",
      "Episode 65000/100000, avg score of 100 evaluation = 7.96\n",
      "Episode 67500/100000, avg score of 100 evaluation = 8.12\n",
      "Episode 70000/100000, avg score of 100 evaluation = 7.98\n",
      "Episode 72500/100000, avg score of 100 evaluation = 7.6\n",
      "Episode 75000/100000, avg score of 100 evaluation = 7.56\n",
      "Episode 77500/100000, avg score of 100 evaluation = 8.06\n",
      "Episode 80000/100000, avg score of 100 evaluation = 7.77\n",
      "Episode 82500/100000, avg score of 100 evaluation = 7.76\n",
      "Episode 85000/100000, avg score of 100 evaluation = 7.72\n",
      "Episode 87500/100000, avg score of 100 evaluation = 7.95\n",
      "Episode 90000/100000, avg score of 100 evaluation = 7.96\n",
      "Episode 92500/100000, avg score of 100 evaluation = 7.9\n",
      "Episode 95000/100000, avg score of 100 evaluation = 7.76\n",
      "Episode 97500/100000, avg score of 100 evaluation = 8.14\n",
      "Episode 100000/100000, avg score of 100 evaluation = 7.69\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent111 = Agent(env=env, learning_rate=0.1, discount=0.7, epsilon=0.3)\n",
    "my_q_table111 = agent111.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = -21.88\n",
      "Episode 5000/100000, avg score of 100 evaluation = -3.83\n",
      "Episode 7500/100000, avg score of 100 evaluation = 5.88\n",
      "Episode 10000/100000, avg score of 100 evaluation = 3.71\n",
      "Episode 12500/100000, avg score of 100 evaluation = 7.94\n",
      "Episode 15000/100000, avg score of 100 evaluation = 7.91\n",
      "Episode 17500/100000, avg score of 100 evaluation = 8.39\n",
      "Episode 20000/100000, avg score of 100 evaluation = 7.92\n",
      "Episode 22500/100000, avg score of 100 evaluation = 7.83\n",
      "Episode 25000/100000, avg score of 100 evaluation = 7.39\n",
      "Episode 27500/100000, avg score of 100 evaluation = 7.85\n",
      "Episode 30000/100000, avg score of 100 evaluation = 8.08\n",
      "Episode 32500/100000, avg score of 100 evaluation = 8.1\n",
      "Episode 35000/100000, avg score of 100 evaluation = 7.88\n",
      "Episode 37500/100000, avg score of 100 evaluation = 7.98\n",
      "Episode 40000/100000, avg score of 100 evaluation = 7.58\n",
      "Episode 42500/100000, avg score of 100 evaluation = 8.37\n",
      "Episode 45000/100000, avg score of 100 evaluation = 7.77\n",
      "Episode 47500/100000, avg score of 100 evaluation = 7.85\n",
      "Episode 50000/100000, avg score of 100 evaluation = 7.96\n",
      "Episode 52500/100000, avg score of 100 evaluation = 7.67\n",
      "Episode 55000/100000, avg score of 100 evaluation = 8.05\n",
      "Episode 57500/100000, avg score of 100 evaluation = 7.87\n",
      "Episode 60000/100000, avg score of 100 evaluation = 7.7\n",
      "Episode 62500/100000, avg score of 100 evaluation = 7.5\n",
      "Episode 65000/100000, avg score of 100 evaluation = 7.77\n",
      "Episode 67500/100000, avg score of 100 evaluation = 7.92\n",
      "Episode 70000/100000, avg score of 100 evaluation = 7.81\n",
      "Episode 72500/100000, avg score of 100 evaluation = 8.03\n",
      "Episode 75000/100000, avg score of 100 evaluation = 8.03\n",
      "Episode 77500/100000, avg score of 100 evaluation = 7.57\n",
      "Episode 80000/100000, avg score of 100 evaluation = 7.89\n",
      "Episode 82500/100000, avg score of 100 evaluation = 7.62\n",
      "Episode 85000/100000, avg score of 100 evaluation = 8.01\n",
      "Episode 87500/100000, avg score of 100 evaluation = 8.07\n",
      "Episode 90000/100000, avg score of 100 evaluation = 8.45\n",
      "Episode 92500/100000, avg score of 100 evaluation = 7.66\n",
      "Episode 95000/100000, avg score of 100 evaluation = 7.9\n",
      "Episode 97500/100000, avg score of 100 evaluation = 7.84\n",
      "Episode 100000/100000, avg score of 100 evaluation = 8.15\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent112 = Agent(env=env, learning_rate=0.1, discount=0.7, epsilon=0.5)\n",
    "my_q_table112 = agent112.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = -4.47\n",
      "Episode 5000/100000, avg score of 100 evaluation = 6.28\n",
      "Episode 7500/100000, avg score of 100 evaluation = 6.76\n",
      "Episode 10000/100000, avg score of 100 evaluation = 8.12\n",
      "Episode 12500/100000, avg score of 100 evaluation = 8.26\n",
      "Episode 15000/100000, avg score of 100 evaluation = 7.87\n",
      "Episode 17500/100000, avg score of 100 evaluation = 7.81\n",
      "Episode 20000/100000, avg score of 100 evaluation = 8.34\n",
      "Episode 22500/100000, avg score of 100 evaluation = 7.76\n",
      "Episode 25000/100000, avg score of 100 evaluation = 7.9\n",
      "Episode 27500/100000, avg score of 100 evaluation = 7.95\n",
      "Episode 30000/100000, avg score of 100 evaluation = 8.24\n",
      "Episode 32500/100000, avg score of 100 evaluation = 7.87\n",
      "Episode 35000/100000, avg score of 100 evaluation = 8.09\n",
      "Episode 37500/100000, avg score of 100 evaluation = 8.2\n",
      "Episode 40000/100000, avg score of 100 evaluation = 7.83\n",
      "Episode 42500/100000, avg score of 100 evaluation = 7.65\n",
      "Episode 45000/100000, avg score of 100 evaluation = 7.76\n",
      "Episode 47500/100000, avg score of 100 evaluation = 7.81\n",
      "Episode 50000/100000, avg score of 100 evaluation = 7.44\n",
      "Episode 52500/100000, avg score of 100 evaluation = 7.92\n",
      "Episode 55000/100000, avg score of 100 evaluation = 8.3\n",
      "Episode 57500/100000, avg score of 100 evaluation = 8.15\n",
      "Episode 60000/100000, avg score of 100 evaluation = 7.8\n",
      "Episode 62500/100000, avg score of 100 evaluation = 7.53\n",
      "Episode 65000/100000, avg score of 100 evaluation = 8.47\n",
      "Episode 67500/100000, avg score of 100 evaluation = 7.82\n",
      "Episode 70000/100000, avg score of 100 evaluation = 7.61\n",
      "Episode 72500/100000, avg score of 100 evaluation = 7.78\n",
      "Episode 75000/100000, avg score of 100 evaluation = 7.83\n",
      "Episode 77500/100000, avg score of 100 evaluation = 7.64\n",
      "Episode 80000/100000, avg score of 100 evaluation = 7.94\n",
      "Episode 82500/100000, avg score of 100 evaluation = 8.0\n",
      "Episode 85000/100000, avg score of 100 evaluation = 7.4\n",
      "Episode 87500/100000, avg score of 100 evaluation = 7.64\n",
      "Episode 90000/100000, avg score of 100 evaluation = 7.65\n",
      "Episode 92500/100000, avg score of 100 evaluation = 8.01\n",
      "Episode 95000/100000, avg score of 100 evaluation = 8.02\n",
      "Episode 97500/100000, avg score of 100 evaluation = 8.05\n",
      "Episode 100000/100000, avg score of 100 evaluation = 7.77\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent113 = Agent(env=env, learning_rate=0.1, discount=0.7, epsilon=0.7)\n",
    "my_q_table113 = agent113.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2500/100000, avg score of 100 evaluation = 8.36\n",
      "Episode 5000/100000, avg score of 100 evaluation = 7.7\n",
      "Episode 7500/100000, avg score of 100 evaluation = 8.21\n",
      "Episode 10000/100000, avg score of 100 evaluation = 7.87\n",
      "Episode 12500/100000, avg score of 100 evaluation = 8.23\n",
      "Episode 15000/100000, avg score of 100 evaluation = 7.77\n",
      "Episode 17500/100000, avg score of 100 evaluation = 7.25\n",
      "Episode 20000/100000, avg score of 100 evaluation = 7.77\n",
      "Episode 22500/100000, avg score of 100 evaluation = 7.99\n",
      "Episode 25000/100000, avg score of 100 evaluation = 7.57\n",
      "Episode 27500/100000, avg score of 100 evaluation = 7.77\n",
      "Episode 30000/100000, avg score of 100 evaluation = 7.82\n",
      "Episode 32500/100000, avg score of 100 evaluation = 8.16\n",
      "Episode 35000/100000, avg score of 100 evaluation = 7.83\n",
      "Episode 37500/100000, avg score of 100 evaluation = 8.49\n",
      "Episode 40000/100000, avg score of 100 evaluation = 7.75\n",
      "Episode 42500/100000, avg score of 100 evaluation = 7.84\n",
      "Episode 45000/100000, avg score of 100 evaluation = 7.94\n",
      "Episode 47500/100000, avg score of 100 evaluation = 8.07\n",
      "Episode 50000/100000, avg score of 100 evaluation = 7.72\n",
      "Episode 52500/100000, avg score of 100 evaluation = 8.27\n",
      "Episode 55000/100000, avg score of 100 evaluation = 8.38\n",
      "Episode 57500/100000, avg score of 100 evaluation = 8.08\n",
      "Episode 60000/100000, avg score of 100 evaluation = 8.06\n",
      "Episode 62500/100000, avg score of 100 evaluation = 7.81\n",
      "Episode 65000/100000, avg score of 100 evaluation = 7.87\n",
      "Episode 67500/100000, avg score of 100 evaluation = 8.25\n",
      "Episode 70000/100000, avg score of 100 evaluation = 7.81\n",
      "Episode 72500/100000, avg score of 100 evaluation = 7.31\n",
      "Episode 75000/100000, avg score of 100 evaluation = 8.08\n",
      "Episode 77500/100000, avg score of 100 evaluation = 7.5\n",
      "Episode 80000/100000, avg score of 100 evaluation = 7.97\n",
      "Episode 82500/100000, avg score of 100 evaluation = 7.91\n",
      "Episode 85000/100000, avg score of 100 evaluation = 8.17\n",
      "Episode 87500/100000, avg score of 100 evaluation = 7.37\n",
      "Episode 90000/100000, avg score of 100 evaluation = 7.76\n",
      "Episode 92500/100000, avg score of 100 evaluation = 7.57\n",
      "Episode 95000/100000, avg score of 100 evaluation = 7.94\n",
      "Episode 97500/100000, avg score of 100 evaluation = 8.09\n",
      "Episode 100000/100000, avg score of 100 evaluation = 7.85\n",
      "learning ended\n"
     ]
    }
   ],
   "source": [
    "agent114 = Agent(env=env, learning_rate=0.1, discount=0.7, epsilon=0.9)\n",
    "my_q_table114 = agent114.q_learnning(epochs=100000, period_of_evaluation=2500, num_of_evaluations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wnioski:\n",
    "z przeprowadzonych eksperymentów można stwierdzić że algorytm Q learning świetnie sobie radzi z poznawaniem i nauką nieznanego wcześniej środowska, jednak ważnym aspektem jest prawidłowy dobór jego parametrów. Zaobserwowałem że zwiększenie każdego z parametrów: *learning rate*, *discount* oraz *epsilon* zmniejsza liczbę iteracji (epok) potrzebą douzyskania satysfakcjonującego średniego wyniku, jednak parametr *epsilon* zdaje się mieć najmniejszy wpływ.<br><br> Warto zaznaczyć że algorytm ten nie jest zasobożerny i nie wymaga dużej liczby iteracji do osiągnięcia dobrego rezultatu działania, przykładowo już dla ok 2500 iteracji uczenia jesteśmy w stanie wytrenować agenta (dla problemu TAXI). <br><br> Niewielkie wachania średniego wyniku (7 - 8) po dłuższym czasie uczenia wynikają z losowości problemu - losowego punktu startowego, końcowego i punktu odbioru pasażera.\n",
    "\n",
    "Wadą algorytmu q lerning jest to że zależy od dobrze zdefiniwanej tablicy Q, przy złożonych problemach, środowiskach może być wymagana bardzo duża liczba iteracji uczących aby uzyskać opytmalne wyniki. Ponadto algorytm ten jest dość prymitywny - po prostu oceniamy każdą możliwą konfigurację środowskia i na tej podstawie wykonujemy działanie.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
